import numpy
import sys
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
import matplotlib as mpl
mpl.use('TkAgg')
import matplotlib.pyplot as plt
from keras import backend as K
K.set_image_dim_ordering('th')


if len(sys.argv) != 4:
    print("Usage: file.py numNeuron epochsNumb batchSize")
    sys.exit()

seed = 7
numpy.random.seed(seed)

numNeuron = int(sys.argv[1])
numClass = 10
pixels = 784

def data_augmentation(model, trainSetX, trainSetY, testSetX, testSetY):
	# reshape to be [samples][pixels][width][height]
	trainSetX = trainSetX.reshape(trainSetX.shape[0], 1, 28, 28)
	testSetX = testSetX.reshape(testSetX.shape[0], 1, 28, 28)
	# convert from int to float
	trainSetX = trainSetX.astype('float32')
	testSetX = testSetX.astype('float32')

	batch_size = 16
	datagen = ImageDataGenerator(zca_whitening=True, rotation_range=90)
	datagen.fit(trainSetX);
	#batch = datagen.flow(trainSetX, trainSetY,batch_size=32)

	#X_batch, y_batch = batch.next()
	# create a grid of 3x3 images
	#for x in X_batch:
	#for i in range(0, 9):
	#	plt.subplot(330 + 1 + i)
	#	plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))
	#	print("number: "+str(y_batch[i]));
	# show the plot
	#plt.show()

	model_aug = model_augmentation(4)

	# Fit the model on the batches generated by datagen.flow().
	history = model_aug.fit_generator(datagen.flow(trainSetX, trainSetY,
	                    batch_size=32),
	                    samples_per_epoch=trainSetX.shape[0],
	                    nb_epoch=10,
	                    validation_data=(testSetX, testSetY))
	#break
	#fit_generator(datagen, samples_per_epoch=len(train), epochs=100)

def plot_history(history):
	# list all data in history
	print(history.history.keys())
	# summarize history for accuracy
	plt.plot(history.history['acc'])
	plt.title('model accuracy')
	plt.ylabel('accuracy')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')
	plt.show()
	# summarize history for loss
	plt.plot(history.history['loss'])
	plt.title('model loss')
	plt.ylabel('loss')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')
	plt.show()

#define model
def model():
    model = Sequential()
    model.add(Dense(numNeuron, input_dim=pixels, activation="relu"))
    model.add(Dropout(0.2, noise_shape=None, seed=seed))
    model.add(Dense(numClass, activation="softmax"))

    #model.add(Dense(1, activation='sigmoid'))
    #optimizer
	adamOptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer=adamOptimizer, metrics=['accuracy'])

    return model

def model_augmentation(input_size):
    model = Sequential()
    model.add(Dense(numNeuron, input_shape=(10000, 1, 28, 28), activation="relu"))
    model.add(Dense(numClass, activation="softmax"))
    #model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

model = model()
# load data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
previous_x = X_train;

data_augmentation(model,X_train, y_train, X_test, y_test);

# flatten 28*28 images to a 784 vector for each image
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')

# normalize inputs from 0-255 to 0-1
X_train = X_train / 255
X_test = X_test / 255

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]


epochsNumb = int(sys.argv[2])
batchSize = int(sys.argv[3])
# Fit the model
history = model.fit(X_train, y_train, epochs=epochsNumb, batch_size=batchSize)

plot_history(history)

# evaluate the model
scores = model.evaluate(X_train, y_train)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
print("\nEpoch: %d, Batch size: %d" % (epochsNumb, batchSize))

print("\nMaking predictions\n")
# calculate predictions
predictions = model.predict(X_test)
# Compare predictions
class_counter = [0] * 10
number_errors = 0
for x,y in zip(predictions,y_test):
    # print wrong predictions
    for i in range(10):
        if round(x[i]) == 1 and round(x[i]) != round(y[i]):
            class_counter[i] = class_counter[i] + 1
            number_errors+=1
            # print("Failed")
            # print("[", end="")
            # for i in range(10):
            #     if i != 9:
            #         print("%d-" % round(x[i]), end="")
            #     else:
            #         print("%d]" % round(x[i]))
            # # print y_test
            # print("[", end="")
            # for i in range(10):
            #     if i != 9:
            #         print("%d-" % round(y[i]), end="")
            #     else:
            #         print("%d]" % round(y[i]))
            # print("\n")
            break

percentage = [x/number_errors for x in class_counter]

#Print file with statistics
thefile = open(''+str(epochsNumb)+"_"+str(numNeuron), 'w')
thefile.write(str(scores)+"\n")
thefile.write(str(percentage))

print("Class error counter: ", end="")
print(class_counter)
